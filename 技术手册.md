# 技术文档

本文档深入介绍了 Graph-Enhanced RAG 系统的技术细节。

## 1. 文档处理 (`graph_rag.py` 中的 `DocumentProcessor` 类)

*   **`process_markdown(file_path)`:**
    *   读取 Markdown 文件。
    *   使用 `markdown` 库将其转换为 HTML。
    *   使用 `BeautifulSoup` 提取纯文本内容。
    *   删除多余的空格。
    *   调用 `_chinese_chunk` 将文本分割成更小的块。

*   **`_chinese_chunk(text, source)`:**
    *   执行中文文本的智能分块，目标是每个块大约 500 个字符。
    *   使用 `jieba` 进行分词。
    *   根据标点符号 (。.！？；) 分割句子。
    *   将句子组合成块，并遵守最大块大小。
    *   返回一个字典列表，每个字典包含一个块的 `content` 和 `source`。

## 2. 向量存储 (`vector_store.py` 中的 `VectorStore` 类)

*   **`__init__(embedding_size, index_path)`:**
    *   使用 `faiss.IndexFlatL2` (L2 距离用于相似性搜索) 初始化 FAISS 索引。
    *   创建一个列表 `id_to_chunk_id` 以将 FAISS 索引 ID 映射到原始块 ID。

*   **`add_item(embedding, chunk_id)`:**
    *   向 FAISS 索引添加单个嵌入向量。
    *   将相应的 `chunk_id` 附加到 `id_to_chunk_id`。

*   **`add_items(embeddings, chunk_ids)`:**
    *   一次添加多个嵌入。
    *   在将嵌入添加到索引之前对其进行归一化。

*   **`search(query_vector, top_k)`:**
    *   归一化查询向量。
    *   使用 FAISS 索引执行相似性搜索。
    *   检索 `top_k` 个最相似的向量。
    *   返回一个 `chunk_id` 列表及其相应的距离。

*   **`save_index(save_path)`:**
    *   将 FAISS 索引保存到磁盘。
    *   还使用 `pickle` 保存 `id_to_chunk_id` 映射。

*   **`load_index(load_path)`:**
    *   从磁盘加载 FAISS 索引和 `id_to_chunk_id` 映射。

*   **`is_empty()`:**
    *   检查 FAISS 索引是否为空。

## 3. Graph RAG (`graph_rag.py` 中的 `GraphRAG` 和 `ImprovedGraphRAG` 类)

*   **`GraphRAG` (基类):**
    *   **`__init__(data_dir, save_dir)`:**
        *   初始化数据和保存目录。
        *   创建 `DocumentProcessor` 和 `SentenceTransformer` (用于嵌入) 的实例。
        *   初始化一个空的有向图 (`nx.DiGraph`)。
        *   设置用于保存图形和相关数据的路径。

    *   **`extract_entities_and_relations(text)`:**
        *   使用 Ollama 和 `deepseek-coder:instruct` 从给定的文本块中提取实体和关系。
        *   向 LLM 发送精心设计的提示，请求 JSON 响应。
        *   包括广泛的错误处理和 JSON 解析逻辑，以处理 LLM 输出的潜在问题 (例如，不完整的 JSON、额外的文本)。
        *   清理和验证提取的实体和关系。
        *   返回实体列表和 (主语, 谓语, 宾语) 关系列表。

    *   **`build_graph(all_chunks, force_rebuild)`:**
        *   从处理后的文本块构建知识图谱。
        *   遍历每个块：
            *   使用 `extract_entities_and_relations` 提取实体和关系。
            *   将实体作为节点添加到图中。
            *   将关系作为边添加到图中。
            *   存储块内容及其嵌入以供以后检索。
        *   将图形和相关数据保存到磁盘。

    *   **`search(query, top_k)`:**
        *   执行基于图的搜索。
        *   计算查询嵌入和实体节点嵌入之间的余弦相似度。
        *   检索 `top_k` 个最相关的实体。
        *   检索与这些实体连接的相关文本块 (使用 "appears_in" 边) 和关系 (使用其他边类型)。

    *   **`generate_answer(query, context, max_tokens)`:**
        *   使用检索到的上下文生成用户查询的答案。
        *   向 Ollama 发送提示，指示它使用提供的上下文并生成简洁、信息丰富的答案。
        *   包括 LLM 调用的错误处理。

    *   **`cosine_similarity(vec1, vec2)`:**
        *   计算两个向量之间的余弦相似度。

    *   **`save_graph()` / `load_graph()`:**
        *   保存和加载知识图谱、节点嵌入和块内容。

*   **`ImprovedGraphRAG` (`GraphRAG` 的子类):**

    *   **`__init__(data_dir, save_dir)`:**
        *   调用 `GraphRAG` 构造函数。
        *   初始化 `VectorStore` 实例。

    *   **`process_documents()`:**
        *   处理 `data_dir` 中的所有 Markdown 文件。
        *   使用 `DocumentProcessor` 提取文本块。
        *   使用 `SentenceTransformer` 为所有块创建嵌入。
        *   将嵌入和块 ID 添加到 `VectorStore`。
        *   使用 `build_graph` 构建知识图谱，并传递提取的块。

    *   **`load()`:**
        *   加载向量索引 (使用 `VectorStore.load_index`) 和知识图谱 (使用 `GraphRAG.load_graph`)。

    *   **`hybrid_search(query, top_k_vector, top_k_graph)`:**
        *   实现混合搜索策略。
        *   **向量搜索：** 使用 `VectorStore.search` 执行向量相似性搜索。
        *   **图搜索：** 查找与从向量搜索获得的块 ID 连接的实体。
        *   **结果融合：** 合并两种搜索方法的结果，按相关性排序，并返回最佳结果。

    *   **`search(query, top_k)`:**
        *   调用 `hybrid_search()`。
        
    *   **`_get_entity_and_relations(self, entity, query_embedding, score_boost)`:**
        *   获取实体信息及其相关关系, 并计算相关性得分

## 4. 主应用程序 (`main.py`)

*   导入 `ImprovedGraphRAG`。
*   设置 `data_dir` 和 `save_dir`。
*   创建 `ImprovedGraphRAG` 的实例。
*   尝试加载现有模型；如果失败，则处理文档以构建模型。
*   进入一个循环：
    *   提示用户输入查询。
    *   使用 `ImprovedGraphRAG.search` 执行搜索。
    *   打印检索到的上下文 (带有来源和相关性)。
    *   使用 `ImprovedGraphRAG.generate_answer` 生成答案。
    *   打印生成的答案。
    *   处理过程中的潜在异常。

## 关键改进和注意事项

*   **混合搜索：** 向量和图搜索的结合提供了一种更强大的检索机制。向量搜索捕获整体语义相似性，而图搜索利用概念之间的关系。
*   **强大的实体/关系提取：** `extract_entities_and_relations` 函数包括全面的错误处理，以处理 LLM 输出的可变性和潜在错误。提示经过精心设计，以鼓励 JSON 输出并专注于相关的实体类型。
*   **智能分块：** `_chinese_chunk` 方法使用分词和标点符号来创建比简单的固定大小拆分更有意义的文本块。
*   **持久性：** 保存和加载向量索引和知识图谱允许系统快速重复使用，而无需重新处理文档。
*   **Ollama 集成：** 使用 Ollama 可以轻松试验不同的本地 LLM。
*   **明确的关注点分离：** 代码被很好地组织成处理特定任务的类 (文档处理、向量存储、图管理、RAG 逻辑)。

这个全面的文档应该可以帮助用户和开发人员有效地理解和使用您的项目。请记住将这些内容另存为项目根目录中的 `README.md` 和 `TECH_DOCS.md`。

### 知识结构化（知识图谱）的实际案例

原文为：

>但这些系统的功效并未完全发挥出来，特别是涉及到经济效率的环节。由于AVC/AGC/稳控，以及自动化远程监控的发展，一线运行人员的需求下降了，但是数据采集与通信、基础数据质量、软件开发维护等岗位的人数需求提高了。数字化系统也有边际效用，随着系统越来越多、系统接入的数据越来越多，数据质量总会有不到位的地方，边际效用递减。如果管理不到位会引起恶性循环，数据质量变差导致大家不愿意用，大家不用会导致数据质量更加失控。现在的AI技术尚不十分成熟，但是未来，我认为其更大的用武之地是配电、营销、设备运维等薄弱环节。对于稳定控制和保护等成熟业务，AI会改进，但只是对现有业务的补充。3 知识建模与AI 可以把软件开发当成知识建模 ，知识包括显性知识（可以形成论文、规程、文档等）；隐性知识（个人经验，难以传授）。事实上，对于有经验的专家，他的全部能力中，隐性知识其实是占据绝大多数的。举一些例子，我以前在500kV变电站工作，当时的站长仅仅通过听电流互感器的声音，就发现了某 互感器存在问题，这个问题甚至影响了调度的数据。站长还曾经带领我们夜间巡视，通过电晕发现某设备的缺陷，可我当时瞪大了眼睛，啥也看不出来。

提取的JSON为：

```JSON
{
    "entities": ["电力系统", "AI技术", "配电", "营销", "设备运维", "稳控", "自动化远程监控", "变电站", "知识建模", "软件开发"],
    "relations": [["电力系统", "包含", "稳控"], ["电力系统", "包含", "自动化远程监控"], ["AI技术", "应用到", "配电"], ["AI技术", "应用到", "营销"], ["AI技术", "应用到", "设备运维"], ["变电站", "属于", "电力系统"], ["知识建模", "结合", "AI技术"], ["知识建模", "包括", "显性知识"], ["知识建模", "包括", "隐性知识"], ["500kV变电站工作", "是", "变电站"]]
}
```

### 大模型提示词

用于知识结构化的提示词为：

```Python
prompt = f"""
        从以下文本中提取电力系统、高性能计算、AI、工程经验、计算机技术等技术相关的实体和关系（重点是电力系统）。
        **请严格按照如下JSON格式输出，不要包含任何额外的文字或说明，不要忘记relations的最后一个"]"符号：**
        {{
            "entities": ["实体1", "实体2", "实体3"],
            "relations": [["实体1", "关系描述", "实体2"], ["实体1", "关系描述", "实体3"],...]
        }}

        文本：{text}
        再次强调,输出必须是一个完整的,有效的JSON对象
        """
```

用于回答问题的提示词为：

```Python
prompt = f"""
        基于以下知识库信息回答问题。要求：
        1. 只回答与电力系统和电力电子技术相关的问题
        2. 综合运用知识库信息和专业知识
        3. 保持专业性和逻辑性
        4. 分点说明，简明扼要

        知识库信息：
        {context_text}

        问题：{query}
        """
```